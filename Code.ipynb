{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install ta\n!pip install tensorflow-gpu==1.14","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"import os, random, ta, time, gc\nimport pandas as pd\nimport numpy as np\nfrom collections import deque\nimport warnings\nwarnings.filterwarnings('once')\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nfrom keras.callbacks.callbacks import CSVLogger, ModelCheckpoint\nfrom keras.callbacks.tensorboard_v1 import TensorBoard\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Dropout, CuDNNLSTM, CuDNNGRU, BatchNormalization, RNN\nfrom keras.layers import Conv1D, TimeDistributed, MaxPooling1D, Flatten\nimport keras.backend as K\n\nfrom hyperopt import fmin, tpe, hp, Trials\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Data():\n    @staticmethod\n    def get(tickers, to_pred):\n        dir_path = '../input/price-volume-data-for-all-us-stocks-etfs/Data/Stocks/'\n        data = []\n        for ticker in tickers:\n            try:\n                df = pd.read_csv(dir_path + ticker)\n            except:\n                continue\n            if df.size < 1000:\n                continue\n            df.drop(['OpenInt'], axis=1, inplace=True)\n            df.set_index('Date', inplace=True)\n            df.sort_index(inplace=True)\n            df.replace({0:np.nan}, inplace=True)\n            df.fillna(method=\"ffill\", inplace=True)\n            df.dropna(inplace=True)\n            classify = lambda current, future: 1 if float(future) > float(current) else 0\n            df['Target'] = list(map(classify, df['Close'], df['Close'].shift(-to_pred)))\n            data.append(df)\n        return data\n    \n    @staticmethod\n    def preprocess(tickers, seq_len, to_pred, features=1, start=1, step=1):\n        X = []\n        y = []\n        chooseFeatures = {\n            1: Data.features1,\n            2: Data.features2,\n            3: Data.features3,\n            4: Data.features4,\n            5: Data.features5,\n            6: Data.features6,\n        }\n        print(chooseFeatures[features].__doc__)\n        print('---------------------')\n        for ticker in tickers:\n            df = Data.get([ticker], to_pred)\n            if not df:\n                continue\n            df = chooseFeatures[features](df[0])\n            sequential_data = Data.create_seq(df, seq_len, start=start, step=step)\n            sequential_data = Data.normalize_movement(sequential_data)\n            # Append sequences and targets of df\n            for seq, target in sequential_data:\n                X.append(seq)\n                y.append(target)\n        return np.array(X), np.array(y)\n    \n    @staticmethod\n    def features1(df_org):\n        '''Percentage change features'''\n        df = df_org.copy()\n        # Normalize and Scale data\n        for col in df.columns:\n            if col != \"Target\":\n                df[col] = df[col].pct_change()\n                df.dropna(inplace=True)\n                df[col] = preprocessing.scale(df[col].values)\n        df.dropna(inplace=True)\n        return df\n    \n    @staticmethod\n    def features2(df_org):\n        '''Close and Volume pct.change features'''\n        df = pd.DataFrame()\n        df['Close'] = df_org['Close']\n        df['Volume'] = df_org['Volume']\n        df['Target'] = df_org['Target']\n        # Normalize and Scale data\n        for col in df.columns:\n            if col != \"Target\":\n                df[col] = df[col].pct_change()\n                df.dropna(inplace=True)\n                df[col] = preprocessing.scale(df[col].values)\n        df.dropna(inplace=True)\n        return df\n\n    @staticmethod\n    def features3(df_org):\n        '''Features without percentage change'''\n        df = df_org.copy()\n        # Normalize and Scale data\n        for col in df.columns:\n            if col != \"Target\":\n                df.dropna(inplace=True)\n                df[col] = preprocessing.scale(df[col].values)\n        df.dropna(inplace=True)\n        return df\n\n    @staticmethod\n    def features4(df_org, n_components=16):\n        '''Denoized pct change features'''\n        df = df_org.copy()\n        # Normalize and Scale data\n        for col in df.columns:\n            if col!='Target' and col!='Volume':\n                df[col] = df[col].ewm(span=10).mean()\n            if col != 'Target':\n                df[col] = df[col].pct_change()\n                df.dropna(inplace=True)\n                df[col] = preprocessing.scale(df[col].values)\n        df.dropna(inplace=True)\n        return df\n\n    @staticmethod\n    def features5(df_org):\n        '''Main technical features'''\n        df = pd.DataFrame(index=df_org.index)\n        df['SMA'] = df_org['Close'].rolling(10).mean()\n        df['EWMA'] = df_org['Close'].ewm(span=10).mean()\n        df['%K'] = ta.momentum.stoch(high=df_org['High'], low=df_org['Low'], close=df_org['Close'])\n        df['%D'] = ta.momentum.stoch_signal(high=df_org['High'], low=df_org['Low'], close=df_org['Close'])\n        df['%R'] = ta.momentum.wr(high=df_org['High'], low=df_org['Low'], close=df_org['Close'])\n        df['MACD'] = ta.trend.macd(close=df_org['Close'])\n        df['RSI'] = ta.momentum.rsi(close=df_org['Close'])\n        df['CCI'] = ta.trend.cci(high=df_org['High'], low=df_org['Low'], close=df_org['Close'])\n        df['ADI'] = ta.volume.acc_dist_index(high=df_org['High'], low=df_org['Low'], \n                                             close=df_org['Close'], volume=df_org['Volume'])\n        df['MOM'] = df_org['Close'].diff(periods=9)\n        df['Target'] = df_org['Target']\n        df.dropna(inplace=True)\n        \n        # Normalize and Scale data\n        for col in df.columns:\n            if col != \"Target\":\n                df[col] = preprocessing.scale(df[col].values)\n        df.dropna(inplace=True)\n        return df\n    \n    @staticmethod\n    def features6(df_org, n_components=16):\n        '''All technical features compressed with PCA'''\n        # Create technical fetures\n        df = ta.add_all_ta_features(df_org.copy(), \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", fillna=True)\n        df = df.reset_index().drop(['Date'], axis=1)\n        # Separate out the features\n        X_tmp = df.drop(['Target'], axis=1)\n        # Standarize the features\n        X_tmp = StandardScaler().fit_transform(X_tmp)\n        # Apply PCA\n        pca = PCA(n_components=n_components)\n        principalComponents = pca.fit_transform(X_tmp)\n        principalDf = pd.DataFrame(data = principalComponents)\n        df = pd.concat([principalDf, df['Target']], axis = 1)\n        return df\n    \n    @staticmethod\n    def create_seq(df, seq_len, start=0, step=1):\n        '''Create sequences'''\n        sequential_data = []\n        if step < 6:\n            prev_days = deque(maxlen=seq_len)\n            for val in df.values:  \n                prev_days.append(val[:-1])\n                if len(prev_days) == seq_len:  \n                    sequential_data.append([np.array(prev_days), val[-1]])\n            if step > 1:\n                sequential_data = sequential_data[start::step]\n        else:\n            for i in range(start, df.shape[0]-seq_len, step):\n                sequential_data.append([\n                    df.iloc[i:i+seq_len].values[:,:-1], \n                    float(df['Target'].iloc[i+seq_len-1])\n                ])\n        random.shuffle(sequential_data)\n        return sequential_data\n    \n    @staticmethod\n    def normalize_movement(sequential_data):\n        '''Normalize distribiution of buys/sells'''\n        buys = []\n        sells = []\n        for seq, target in sequential_data: \n            if target == 0:\n                sells.append([seq, target])  \n            elif target == 1:\n                buys.append([seq, target])\n        random.shuffle(buys)\n        random.shuffle(sells)\n        \n        lower = min(len(buys), len(sells))\n        buys = buys[:lower]\n        sells = sells[:lower]\n        sequential_data = buys + sells\n        random.shuffle(sequential_data)\n        return sequential_data\n    \n    @staticmethod\n    def split(X, y, test_size=0.2):\n        testing = int(test_size * len(X))\n        X_train = X[:-testing]\n        y_train = y[:-testing]\n        X_test = X[-testing:]\n        y_test = y[-testing:]\n        return X_train, y_train, X_test, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model_LSTM:\n    def __init__(self, name='model'):\n        self.name = name \n        self.csv_logger = CSVLogger(self.name+'.csv', separator=',', append=True)\n        self.checkpoint = ModelCheckpoint(\n            self.name+'.hdf5',\n            monitor='val_acc',\n            verbose=0,\n            save_best_only=True,\n            mode='max'\n        )\n        \n    def build(self, input_shape, lr=3e-5, n_RN=128, n_DN=32, dropoutR=0.2, dropoutD=0.2):\n        model = Sequential()\n        model.add(CuDNNLSTM(n_RN, input_shape=(input_shape), return_sequences=True))\n        model.add(Dropout(dropoutR))\n        model.add(BatchNormalization()) \n\n        model.add(CuDNNLSTM(n_RN, return_sequences=True))\n        model.add(Dropout(dropoutR))\n        model.add(BatchNormalization())\n        \n        model.add(CuDNNLSTM(n_RN))\n        model.add(Dropout(dropoutR))\n        model.add(BatchNormalization())\n\n        model.add(Dense(n_DN, activation='relu'))\n        model.add(Dropout(dropoutD))\n\n        model.add(Dense(2, activation='softmax'))\n        \n        # Compile model\n        model.compile(\n            loss='sparse_categorical_crossentropy',\n            optimizer=Adam(lr=lr, decay=lr*1e-3),\n            metrics=['acc'],\n        )\n        self.model = model\n        \n    def train(self, X, y, batch_size=32 ,epochs=1, verbose=0):\n        self.model.fit(\n            X, y,\n            batch_size=batch_size,\n            epochs=epochs,\n            verbose=verbose,\n            callbacks=[self.csv_logger, self.checkpoint],\n        )\n    \n    def train_test(self, X_train, y_train, X_val, y_val, batch_size=32 ,epochs=1 ,verbose=2, visualize=False):\n        history = self.model.fit(\n            X_train, y_train,\n            batch_size=batch_size,\n            epochs=epochs,\n            validation_data=(X_val, y_val),\n            verbose=2,\n            callbacks=[self.csv_logger, self.checkpoint],\n        )\n        \n        if visualize:\n            fig, axarr = plt.subplots(1, 2, figsize=(16, 8))\n            # Plot training & validation accuracy values\n            axarr[0].plot(history.history['acc'])\n            axarr[0].plot(history.history['val_acc'])\n            axarr[0].set_title('Model accuracy')\n            axarr[0].set_ylabel('Accuracy')\n            axarr[0].set_xlabel('Epoch')\n            axarr[0].legend(['Train', 'Test'], loc='upper left')\n\n            # Plot training & validation loss values\n            axarr[1].plot(history.history['loss'])\n            axarr[1].plot(history.history['val_loss'])\n            axarr[1].set_title('Model loss')\n            axarr[1].set_ylabel('Loss')\n            axarr[1].set_xlabel('Epoch')\n            axarr[1].legend(['Train', 'Test'], loc='upper left')\n            \n            plt.show()\n\n    def evaluate(self, X, y, is_print=True):\n        score = {}\n        eval_score = self.model.evaluate(X, y, verbose=0)\n        score['loss'] = eval_score[0]\n        score['acc'] = eval_score[1]\n        if is_print:\n            print(score)\n        return score\n    \n    def save(self, name):\n        self.model.save(name)\n        \n    def load(self, name):\n        self.model = load_model(name)\n        \n        \nclass Model_GRU(Model_LSTM):   \n    def build(self, input_shape, lr=3e-5, n_RN=128, n_DN=32, dropoutR=0.2, dropoutD=0.2):\n        model = Sequential()\n        model.add(CuDNNGRU(n_RN, input_shape=(input_shape), return_sequences=True))\n        model.add(Dropout(dropoutR))\n        model.add(BatchNormalization()) \n\n        model.add(CuDNNGRU(n_RN, return_sequences=True))\n        model.add(Dropout(dropoutR))\n        model.add(BatchNormalization())\n        \n        model.add(CuDNNGRU(n_RN))\n        model.add(Dropout(dropoutR))\n        model.add(BatchNormalization())\n\n        model.add(Dense(n_DN, activation='relu'))\n        model.add(Dropout(dropoutD))\n\n        model.add(Dense(2, activation='softmax'))\n        \n        # Compile model\n        model.compile(\n            loss='sparse_categorical_crossentropy',\n            optimizer=Adam(lr=lr, decay=lr*1e-3),\n            metrics=['acc'],\n        )\n        self.model = model\n        \n        \nclass Model_RNN(Model_LSTM):   \n    def build(self, input_shape, lr=3e-5, n_RN=128, n_DN=32, dropoutR=0.2,dropoutD=0.2):\n        model = Sequential()\n        model.add(CuDNNRNN(n_RN, input_shape=(input_shape), return_sequences=True))\n        model.add(Dropout(dropoutR))\n        model.add(BatchNormalization()) \n\n        model.add(CuDNNRNN(n_RN, return_sequences=True))\n        model.add(Dropout(dropoutR))\n        model.add(BatchNormalization())\n        \n        model.add(CuDNNRNN(n_RN))\n        model.add(Dropout(dropoutR))\n        model.add(BatchNormalization())\n\n        model.add(Dense(n_DN, activation='relu'))\n        model.add(Dropout(dropoutD))\n\n        model.add(Dense(2, activation='softmax'))\n        \n        # Compile model\n        model.compile(\n            loss='sparse_categorical_crossentropy',\n            optimizer=Adam(lr=lr, decay=lr*1e-3),\n            metrics=['acc'],\n        )\n        self.model = model\n        \nclass Model_CLSTM(Model_LSTM):   \n    def build(self, input_shape, lr=8e-5, n_F=64, kS=3, pS=2, n_RN=220, n_DN=120, dropoutC=0.3, dropoutR=0.3, dropoutD=0.2):\n        model = Sequential()\n        model.add(Conv1D(filters=n_F, kernel_size=kS, activation='relu', data_format=\"channels_last\", input_shape=(input_shape)))\n        model.add(Dropout(dropoutC))\n        model.add(MaxPooling1D(pool_size=pS))\n        \n        model.add(Conv1D(filters=n_F, kernel_size=kS, activation='relu'))\n        model.add(Dropout(dropoutC))\n        model.add(MaxPooling1D(pool_size=pS))\n        \n        model.add(CuDNNLSTM(n_RN, return_sequences=True))\n        model.add(Dropout(dropoutR))\n        model.add(BatchNormalization()) \n\n        model.add(CuDNNLSTM(n_RN, return_sequences=True))\n        model.add(Dropout(dropoutR))\n        model.add(BatchNormalization())\n        \n        model.add(CuDNNLSTM(n_RN))\n        model.add(Dropout(dropoutR))\n        model.add(BatchNormalization())\n\n        model.add(Dense(n_DN, activation='relu'))\n        model.add(Dropout(dropoutD))\n\n        model.add(Dense(2, activation='softmax'))\n        \n        # Compile model\n        model.compile(\n            loss='sparse_categorical_crossentropy',\n            optimizer=Adam(lr=lr, decay=lr*1e-3),\n            metrics=['acc'],\n        )\n        self.model = model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def hyperparameter_tuning():\n    def objective(space):\n        model = Model_LSTM(name='name')\n        model.build(\n            input_shape=input_shape,\n            lr=space['lr'],\n            n_RN=int(space['n_RN']),\n            n_DN=int(space['n_DN']),\n            dropoutR=space['dropoutR'],\n            dropoutD=space['dropoutD'],\n        )\n        model.train(\n            X,\n            y,\n            batch_size=int(space['batch_size']),\n            epochs=1,\n        )\n        score = model.evaluate(X_val, y_val)\n        return score['loss']\n\n    np.random.seed(0)\n    tickets = os.listdir(\"../input/price-volume-data-for-all-us-stocks-etfs/Data/Stocks\")\n    random.shuffle(tickets)\n    train_tickets = tickets[:-300]\n    validation_tickets = tickets[-300:]\n    data = Data.get(train_tickets[:300], to_pred=3)\n    val_data = Data.get(validation_tickets, to_pred=3)\n    data[0].head()\n\n    X, y = Data.preprocess(\n        data, \n        seq_len=60,\n        features=1\n    )\n    X_val, y_val = Data.preprocess(\n        val_data, \n        seq_len=60,\n        features=1\n    )\n    input_shape = X.shape[1:]\n\n    space = {\n        'lr': hp.uniform('lr', 1e-6, 1e-4),\n        'n_RN': hp.quniform('N_RN', 128, 256, 1),\n        'n_DN': hp.quniform('n_DN', 32, 256, 1),\n        'dropoutR': hp.uniform('dropoutR', 0.1, 0.5),\n        'dropoutD': hp.uniform('dropoutD', 0.1, 0.5),\n        'batch_size': hp.quniform('batch_size', 32, 64, 1),\n    }\n\n    trials = Trials()\n    best = fmin(\n        fn=objective,\n        space=space,\n        algo=tpe.suggest,\n        max_evals=40,\n        trials=trials,\n    )\n\n    print('----------BestValues----------')\n    for key, val in best.items():\n        print(f'{key}: {val}')\n    print('-----------------------------')\n\n    # Create empty tpe_results dictionary\n    tpe_results = {'loss': []}\n    for key in trials.trials[0]['misc']['vals']:\n        tpe_results[key] = []\n    # Fill tpe_results\n    for trial in trials.trials:\n        tpe_results['loss'].append(trial['result']['loss'])\n        for key, val in trial['misc']['vals'].items():\n            tpe_results[key].append(val[0])\n\n    tpe_results_df=pd.DataFrame(tpe_results)\n    tpe_results_df.to_csv('tpe_results.csv', index=False)\n    tpe_results_df.plot(subplots=True, figsize=(10, 10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(train_data, validation_data, params, features=1, model='LSTM'):\n    X_train, y_train = Data.preprocess(train_data, \n                           seq_len=params['seq_len'],\n                           features=features)\n    X_validate, y_validate = Data.preprocess(validation_data, \n                                             seq_len=params['seq_len'],\n                                             features=features)\n    input_shape = X_train.shape[1:]\n   \n    model_name = 'LSTM'\n    Model = Model_LSTM\n    if model == 'GRU':\n        model_name = 'GRU'\n        Model = Model_GRU\n    if model == 'RNN':\n        model_name = 'RNN'\n        Model = Model_RNN\n    if model == 'CLSTM':\n        model_name == 'CLSTM'\n        Model = Model_CLSTM\n    print(model_name)\n    print(Model)\n    name = f\"{model_name}-f{features}-sl{params['seq_len']}-tp{params['to_pred']}\"\n    model = Model(name=name)\n    model.build(\n        input_shape=input_shape,\n        lr=params['lr'],\n        n_RN=params['n_RN'],\n        n_DN=params['n_DN'],\n        dropoutR=params['dropoutR'],\n        dropoutD=params['dropoutD'],\n    )\n    model.train_test(\n        X_train,\n        y_train,\n        X_validate,\n        y_validate,\n        batch_size=params['batch_size'],\n        epochs=params['epochs'],\n        visualize=True,\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def repetitiveness_test():\n    # params = {\n    #     'seq_len': 60,\n    #     'to_pred': 3,\n    #     'batch_size': 45,\n    #     'epochs': 10,\n    #     'n_RN': 152,\n    #     'n_DN': 231,\n    #     'dropoutR': 0.17,\n    #     'dropoutD': 0.39,\n    #     'lr': 4.4e-5,\n    # }\n    params = {\n        'seq_len': 60,\n        'to_pred': 3,\n        'batch_size': 32,\n        'epochs': 10,\n        'n_RN': 128,\n        'n_DN': 32,\n        'dropoutR': 0.3,\n        'dropoutD': 0.2,\n        'lr': 4e-5,\n    }\n    random.seed(0)\n    tickets = os.listdir(\"../input/price-volume-data-for-all-us-stocks-etfs/Data/Stocks\")\n    random.shuffle(tickets)\n    random.shuffle(tickets)\n    train_tickets = tickets[:-300]\n    random.shuffle(train_tickets)\n    data = Data.get(train_tickets[:500], to_pred=params['to_pred'])\n    X, y = Data.preprocess(\n        data, \n        seq_len=params['seq_len'],\n        features=1,\n    )\n    val_tickets = tickets[-300:]\n    val_data = Data.get(val_tickets, to_pred=params['to_pred'])\n    X_val, y_val = Data.preprocess(\n        val_data, \n        seq_len=params['seq_len'],\n        features=1,\n    )\n    input_shape = X.shape[1:]\n    for i in range(5):\n        random.seed(int(time.time()))\n        model = Model_LSTM()\n        model.build(\n            input_shape=input_shape,\n            lr=params['lr'],\n            n_RN=params['n_RN'],\n            dropoutR=params['dropoutR'],\n            n_DN=params['n_DN'],\n            dropoutD=params['dropoutD'],\n        )\n        model.train_test(\n            X,\n            y,\n            X_val,\n            y_val,\n            batch_size=params['batch_size'],\n            epochs=params['epochs'],\n            visualize=True,\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing_methods_test():\n    params = {\n        'seq_len': 60,\n        'to_pred': 3,\n        'batch_size': 45,\n        'epochs': 3,\n        'n_RN': 152,\n        'n_DN': 231,\n        'dropoutR': 0.17,\n        'dropoutD': 0.39,\n        'lr': 4.4e-5,\n    }\n\n    np.random.seed(10)\n    tickers = os.listdir(\"../input/price-volume-data-for-all-us-stocks-etfs/Data/Stocks\")\n    random.shuffle(tickers)\n    train_tickers = tickers[:-300]\n    random.shuffle(train_tickers)\n    data = Data.get(train_tickers[:500], to_pred=params['to_pred'])\n    val_tickers = tickers[-300:]\n    val_data = Data.get(val_tickers, to_pred=params['to_pred'])\n\n    for features in range(1, 4):\n        test(data, val_data, params, features=features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def horizon_test():\n    params = {\n        'seq_len': 60,\n        'to_pred': 3,\n        'batch_size': 45,\n        'epochs': 10,\n        'n_RN': 152,\n        'n_DN': 231,\n        'dropoutR': 0.17,\n        'dropoutD': 0.39,\n        'lr': 4.4e-5,\n    }\n\n    np.random.seed(0)\n    tickers = os.listdir(\"../input/price-volume-data-for-all-us-stocks-etfs/Data/Stocks\")\n    random.shuffle(tickers)\n    random.shuffle(tickers)\n    train_tickers = tickers[:500]\n    random.shuffle(train_tickers)\n    val_tickers = tickers[-300:]\n\n    # test seq_len in [20, 60, 120]\n    params['seq_len'] = 120\n    for to_pred in [10, 15]:\n        params['to_pred'] = to_pred\n        train_data = Data.get(train_tickers, to_pred=params['to_pred'])\n        val_data = Data.get(val_tickers, to_pred=params['to_pred'])\n        print(f\"Sequence Length: {params['seq_len']}\")\n        print(f\"To Predict: {params['to_pred']}\")\n        test(train_data, val_data, params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_final_model():\n    X, y = [], []\n    step = 10\n    starts = [i for i in range(step)]\n    for lr in [1e-6, 1e-6]:\n        i = 0\n        random.shuffle(starts)\n        K.set_value(model.model.optimizer.lr, lr)\n        for start in starts:\n            i += 1\n            print('Iter: ', i)\n            print('Day: ', start)\n            del X, y\n            gc.collect()\n            X, y = Data.preprocess(\n                train_tickers,\n                seq_len=params['seq_len'],\n                to_pred=params['to_pred'],\n                features=1,\n                start=start,\n                step=step\n            )\n            model.train_test(\n                X,\n                y,\n                X_val,\n                y_val,\n                batch_size=params['batch_size'],\n                epochs=params['epochs'],\n            )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}